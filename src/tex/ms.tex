% Define document class
\documentclass[]{aastex631}
\usepackage{showyourwork}
\usepackage{macros}
\usepackage{amsmath}
\usepackage{enumerate}

% Begin!
\begin{document}

% Title
\title{No need to know: astrophysics-agnostic spectral siren cosmology}
%\title{No need to know: gravitational-wave cosmology with an astrophysics-agnostic mass distribution}
%\title{No need to know: spectral siren cosmology with an astrophysics-agnostic mass distribution of gravitational-wave sources}
%\title{No need to know: gravitational-wave cosmology while astrophysically ignorant/for the astrophysically-ignorant}

% Author list
%\author{@afarah18}
\author{Amanda M. Farah}
\email{afarah@uchicago.edu}
\affiliation{Department of Physics, University of Chicago, Chicago, IL 60637, USA}

\author{Thomas A. Callister}
\affiliation{Kavli Institute for Cosmological Physics, The University of Chicago, Chicago, IL 60637, USA}

\author{Jose Mar\'ia Ezquiaga}
%\email{jose.ezquiaga@nbi.ku.dk}
\affiliation{Niels Bohr International Academy, Niels Bohr Institute, Blegdamsvej 17, DK-2100 Copenhagen, Denmark}

\author{Michael Zevin}
\affiliation{The Adler Planetarium, 1300 South DuSable Lake Shore Drive, Chicago, 60605, IL, USA}

\author{Daniel E. Holz}
\affiliation{Kavli Institute for Cosmological Physics and Enrico Fermi Institute, The University of Chicago, Chicago, IL 60637, USA}
\affiliation{Department of Physics, Department of Astronomy \& Astrophysics, The University of Chicago, Chicago, IL 60637, USA}


% Abstract
\begin{abstract}
Gravitational waves (GWs) are unique transients in which they encode direct information about their luminosity distance. 
Adding redshift, a GW standard siren Hubble diagram can be constructed to probe the standard cosmological model.  
Although individual signals do not have any reference scale imprinting a redshift timestamp, as a collective there could be signatures in their mass spectrum that would break the mass-redshift degeneracy. 
This spectral siren method thus provides a GW data-only approach to constrain the Universe's expansion rate from the population of compact binary coalescences. 
So far, this technique has only been applied with simple parametric representations of the mass distribution. 
However, the use of a wrong mass model would inevitably lead to biases in the cosmological inference, an acute problem given our current lack of understanding of the true astrophysical model.
We demonstrate that the spectral siren method is purely data-driven, in which the right cosmology can be inferred without a given model, using a flexible Gaussian process reconstruction of the mass spectrum. 
In particular, we analyze an observing scenario with XX GW signals and detectors at XX sensitivity. 
We find that both the source mass model and the cosmological parameters are correctly reconstructed. 
This astrophysics-agnostic spectral siren technique will be key to deliver precise and unbiased cosmological constraints with future observations in the presence of possible redshift evolution of the mass spectrum.  
\end{abstract}

\section{}
\textcolor{red}{Showyourwork Guidelines:}
\begin{itemize}
    \item Always edit \texttt{ms.tex} in Overleaf
    \item Always edit \texttt{bib.bib} in Overleaf
    \item Always edit \texttt{macros.sty} in Overleaf
    \item Always edit anything in the \texttt{figures} directory using git (with a python script that should generate the figures)
    \item Always edit anything in the \texttt{output} directory using git (with a python script that should generate the output)
\end{itemize}

\section{Introduction}
\label{sec:intro}
Like light, gravitational waves (GWs) are redshifted as they propagate across the universe.
Unlike light, however, the selection effects of GWs are extremely well understood, allowing for a precise estimate of each catalogs' completeness and an unbiased measurement of the true GW population.
This allows GW signals to be used as relatively clean probes of cosmological parameters \citep[e.g.][]{}%bright sirens first paper, 170817 bright siren, dark siren first paper, dark siren with 0817, hithhikers guide.

GW signals provide direct measurements of each source's luminosity distance and redshifted, or detector frame, masses, $m_{\det}= m_{\source}(1+z)$.
Therefore, if the source frame mass is known, each event provides a direct mapping between luminosity distance and redshift, allowing for a measurement of the rate of expansion of the universe at the time the GW signal was emitted, $H(z)$.
In practice, the source frame mass is unknown, but it is possible to know the location of features in the source frame mass distribution.
The whole mass distribution therefore acts similarly to an electromagnetic spectrum, where the location of spectral features provides a redshift measurement.
The method of using the mass distribution of GW sources to measure cosmological parameters has therefore been coined ``spectral sirens'' \citep{ezquiaga_spectral_2022}.
Spectral sirens were first demonstrated to be a feasible method to measure the Hubble constant by \cite{chernoff+fin} using the BNS mass distribution, and extended to the BBH mass distribution by \cite{farr_future_2019}.

Perhaps because of the analogy to electromagnetic spectra in which the location of the source frame spectral features is well-known, or because of the examples given in \cite{farr_future_2019} and \cite{chernoff+fin}, it is commonly presumed that the exact shape of the mass distribution must be known \emph{a priori} to do this measurement, from e.g. theoretical expectations of the location of a pulsational-pair instability pileup, upper mass gap, or maximum neutron star mass.
However, the information in the spectral siren measurement comes solely from the assumption that, at a given luminosity distance, all CBC mergers follow the same mass distribution.
Therefore the location, shape, and existence of all features in the mass distribution can be inferred simultaneously with $H(z)$.

In this work, we explicitly demonstrate that no \emph{a priori} knowledge about the shape of the CBC mass spectrum is necessary to use the spectral siren methodology. 
We do this by inferring \Ho with a flexible, non-parametric model for the mass distribution of CBCs. 
This model makes minimal prior assumptions about the shape of the mass distribution, enabling it to accurately infer a wide range of morphologies.
Despite its flexibility, it is able to consistently obtain an unbiased measurement for \Ho, showing that nonparametric methods are not only sufficient for a spectral siren measurement, they can also mitigate systematic effects in the measurement caused by model misspecification.

We find that using this model allows for a \result{X}\% measurement of \Ho by the end of the LVK's fifth observing run, and a \result{X}\% measurement with only \result{X} weeks of observing with a next-generation detector network. 
While constraints on \Ho are used in this work to quantify the accuracy and precision of our non-parametric method, we emphasize that spectral sirens are able to constrain $H(z)$ out large redshifts that are inaccessible by electromagnetic observations, especially with next-generation detectors \citep{ezquiaga_spectral_2022, Chen:2024gdn}\footnote{See example calculations here: \href{https://github.com/ezquiaga/spectral_sirens}{https://github.com/ezquiaga/spectral\_sirens}}.

\jme{[cite joint multi-siren forecast \cite{Chen:2024gdn}. Cite code \href{https://github.com/ezquiaga/spectral_sirens}{https://github.com/ezquiaga/spectral\_sirens}?]}\\ 
\comment{[Should I add a separate paragraph for other sirens or is it sufficient to just cite it above with other forecasts, as I did?]}

\section{Methods I: Spectral Siren Cosmology}
\label{sec:ss}
\begin{enumerate}
    \item Briefly explain HBA with selection effects.
\end{enumerate}
Given GW data $\{d\}$ that contains $N_{\text{evs}}$ detected GW signals, each with parameters $\theta_i$, the posterior probability of the GW sources being drawn from an underlying population and background cosmology described by hyper-parameters $\Lambda$ is \citep{loredo, taylor, mandel}
\begin{equation}
    p(\Lambda|d) \propto p(\Lambda) e^{-N_{\text{exp}}(\Lambda)} \prod_i^{N_{\text{evs}}} \frac{\diff N}{\diff t_{\det} \diff \theta_i} (\theta_i;\Lambda) .
    \label{eq:inhomog-poisson}
\end{equation}
Here, $N_{\text{exp}(\Lambda)}$ is the expected number of detections given $\Lambda$ and the GW detector sensitivity, calculated using a Monte Carlo sum over $N_{\text{inj}}$ found signals injected into the data stream \citep[see][for a detailed explanation of this process]{essick}.
$\frac{\diff N}{\diff t_{\det} \diff \theta} (\theta;\Lambda)$ is the detector frame merger rate density.
This quantity is the population model conditioned on hyper-parameters $\Lambda$. Following \citet{callister_parameter_2023}, we use a semicolon to explicitly indicate that this is a function of $\Lambda$, not a density over $\Lambda$.

Since the parameters of each event are not perfectly measured, we marginalize $\frac{\diff N}{\diff t_{\det} \diff \theta} (\theta;\Lambda)$ over the event parameters' likelihood given the data.
Practically, this is done by a Monte Carlo average over each event's posterior samples $\{\theta_j\}_i$, dividing out the prior used when inferring those posterior samples, $\pi_{PE}(\theta)$:
\begin{equation}
    \frac{\diff N}{\diff t_{\det} \diff \theta_i} (\theta_i;\Lambda) = \frac{1}{N_{\text{samples}}}\sum_j^{N_{\text{samples}}} \frac{\diff N}{\diff t_{\det} \diff \theta_{i,j}} (\theta_{i,j};\Lambda) \frac{1}{\pi_{PE}(\theta_{i,j})} .
    \label{eq:single-event-likelihood}
\end{equation}
Finally, $p(\Lambda)$ is the prior on the population and parameters on the background cosmology, which we discuss in further detail below.
We infer the posterior in Equation~\ref{eq:inhomog-poisson} using the no-u-turn sampler for Hamiltonian Monte Carlo within \texttt{numpyro} \citep{hoffman_no-u-turn_2011, numpyro}. 

We only consider masses and redshifts, neglecting spins as they do not contain cosmological information.
Thus, $\theta = \{m_1,m_2,z\}$.
We assume a uniform-in-comoving volume redshift distribution, as in Equation~\ref{eq:underlying redshift dist}.
We assume $m_1=m_2$ and model the distribution of primary masses using a Gaussian process (described in Section~\ref{sec:model}.

The spectral siren method of measuring cosmological parameters relies on the assumption that all compact binaries follow the same mass distribution regardless of redshift\footnote{\citet{ezquiaga_spectral_2022} show that the source frame masses can follow different distributions at different redshifts, so long as this redshift evolution of the mass distribution does not exactly mimic cosmology (i.e. all features cannot exhibit a monotonic rightward shift with redshift).}.
The likelihood in Equation~\ref{eq:inhomog-poisson} is therefore maximized when the relationship between redshift and luminosity distance makes all source frame masses follow the same distribution.
This is the basis of the spectral siren method of measuring cosmological parameters; we can simultaneously infer the relationship between redshift and luminosity distance with the parameters of the source frame mass distribution.
The full set of hyper-parameters therefore includes the cosmological parameters that dictate the $D_L$-$z$ relation: \Ho, \Omm, $\Omega_\Lambda, \Omega_r,$ and $w$.
In this work, we fix $\Omega_\Lambda=1-\Omega_M, \Omega_r=0$ and $w=1$ and use uniform priors on \Ho{} and \Omm{}.
Algorithmically, this is equivalent to drawing a value of \Ho, using it to define a relationship between luminosity distance and redshift, then transforming detector frame masses to source frame masses according to $m_{\det} (1+z(D_L)) = m_{\source}$, evaluating the source-frame mass distribution at these transformed values, and then evaluating the likelihood in Equation~\ref{eq:inhomog-poisson}.
The full process is outlined in Section~\ref{sec:model}.

\section{Methods II: Gaussian process-based mass distribution}
\label{sec:model}
Gaussian processes (GPs) are random processes for which any linear combination of outcomes are Gaussian distributed \citep{rasmussen_gaussian_2006}.
Their smoothness properties make them widely useful in GW data analysis and beyond for regression problems, such as modeling time-domain waveforms \citep{zoheyr, huerta}, density estimation problems, such as estimating posterior densities of single-event parameters from parameter estimation samples ~\citep{dangelo}, and as a prior on histogram bin heights \citep{ray_2023,li_flexible_2021}.
Our use case is slightly different.
We will utilize a GP as a prior on the functions that describe the primary mass distribution of CBCs.
It will therefore replace $p(\Lambda)$ in Equation~\ref{eq:inhomog-poisson}.
This choice adds very little prior information about the shape of the mass distribution, besides enforcing that it must be smooth.
Practically, the only difference in the inference of the population when using a GP versus other modeling choices is that $\frac{\diff N}{\diff t_{\det} \diff \theta_i} (\theta_i;\Lambda)$ in Equation~\ref{eq:inhomog-poisson} is determined directly by a realization of the GP, rather than by a handful of hyper-parameters $\Lambda$ and evaluated on an analytical function.

In other words, when using parametric models, $\frac{\diff N}{\diff t_{\det} \diff \theta_i} (\theta_i;\Lambda)$ in Equation~\ref{eq:inhomog-poisson} is calculated by evaluating a specific functional form described by a small set of hyper-parameters. 
With the GP approach, the hyper-parameters describing the mass distribution are the rate at each event-level posterior sample's source frame mass, and the rate at each found injection's source frame mass.
% The source frame mass is different for each possible value of \Ho{} and \Omm{}, so the hyper-parameters are defined in different locations for each draw from the hyper-posterior.
We therefore have $N_{\text{evs}}M + N_{\text{inj}}$ mass hyper-parameters, where $N_{\text{ev}}$ is the number of events, $M$ is the number of PE samples per event, and $N_{\text{inj}}$ is the number of injections used to calculate the selection function $\xi$.
We label these $\{\mathcal{R}_ij,\mathcal{R}_k\}$, and our full set of hyper-parameters is $\Lambda=\{H_0, \Omega_M, \mathcal{R}_ij,\mathcal{R}_k\}$. 
In this way, our GP-based mass distribution is similar to the autoregressive population models used in ~\citet{callister_ar}.
Indeed, an autoregressive model is a Gaussian process with a particular choice of kernel.

A kernel is a function that defines the covariance between input points in the GP (in our case, two source frame mass values). 
It defines the notion of similarity between adjacent points and thereby encodes our assumptions about the smoothness of the source frame mass distribution \citep{rasmussen_gaussian_2006}.
We use a Mat\'ern kernel \citep{handcock_and_stein,stein_1999} with $\nu = 5/2$, but have repeated the analysis with $\nu=3/2$ and $\infty$, finding little impact on the results, except that the $\nu=\infty$ case (also called the ``squared exponential'' kernel) produces a slightly more jagged mass distribution.
Mat\'ern kernels have two parameters besides the mean variance that determine their properties: a length scale $l$ and variance $s$.
In our use case, these are one level further removed from hyper-parameters, so we adopt the terminology used in \citet{callister_ar} and call them ``hyper-hyper-parameters.''
We fit these hyper-hyper-parameters along with the hyper-parameters $\Lambda$ to minimize prior assumptions about the form of the mass distribution.
We use ``penalized-complexity'' priors on the hyper-hyper-parameters to enforce that the model does not create small-scale structure uninformed by data, thereby avoiding over-fitting \citep{simpson_penalising_2017,simpson_garcpas_2022}. % remove this if we don't use it.
Explicitly, the priors on $l$ and $s$ have less than 5\% support for correlation lengths smaller than the \result{minimum} spacing between event-level posterior samples.

GP evaluations notoriously scale as the cube of the number of data points, making them unwieldy with large data sets such as the ones expected for O5.
We therefore make two approximations to a full GP to increase computational efficiency.
First, for each likelihood evaluation, we evaluate a full GP on a regular grid between $0\Msun$ and $250\Msun$ and then interpolate it at each data point.
Second, we use the quasi-separability of Mat\'ern kernels to analytically perform the transformation between covariance matrix and GP draw.
This second step is done using the \texttt{QuasisepSolver} module \citep{foreman-mackey_fast_2017} in the \texttt{tinygp} code base, and requires data to be sort-able (i.e. one-dimensional).

Algorithmically, each hyper-likelihood evaluation contains the following steps:
\begin{enumerate}
    \item Draw cosmological parameters \Ho{} and \Omm{} from the prior distributions described in Equation~\ref{eq:cosmo-priors}.
    \item Transform the luminosity distances and detector frame masses of each event posterior sample and injection into redshifts and source frame masses according to the cosmology specified by step 1.
    \item Draw hyper-hyper-parameters $l$ and $s$ from the penalized-complexity priors described above.
    \item Draw a single GP realization with a kernel defined by $l$ and $s$. This is defined on a regular grid of source-frame masses and evaluated using the \texttt{QuasisepSolver}.
    \item Interpolate the GP at each event posterior sample and injection source frame mass (from step 2)
    \item Calculate the population likelihood according to Equation~\ref{eq:inhomog-poisson}.
\end{enumerate}

\jme{[advertise code is public!]}


\section{Results}
\label{sec:results}

\subsection{Option 1: include parametric models}
In this Section, we first show that fitting an incorrect functional form to the mass distribution of CBCs biases the inference of cosmological parameters when using the spectral siren methodology.
We then demonstrate that our flexible model alleviates this bias without the need to know the mass distribution's morphology \emph{a priori}.
To illustrate this, we generate a catalog of \variable{output/num_found_events.txt}GW events from an underlying mass distribution described by the \plp{} model, with hyperparameters consistent with those found in \citet{o3b_pop}.
This simulated catalog is designed to be representative of the gravitational wave transient catalog after the LVK's fifth observing run.
The details of the data simulation are described in Appendix~\ref{ap:data generation}.
We then use this catalog to infer \Ho{} using three different models for the source-frame mass distribution:
\begin{enumerate}
    \item the \plp{} model, which includes the true mass distribution within its hyperprior
    \item a broken power law, which does not include the true mass distribution within its hyperprior, and 
    \item the flexible, Gaussian process-based model described in Section~\ref{sec:model}, which can closely approximate the morphology of the true mass distribution, along with several other morphologies.
\end{enumerate}
The results of each of these fits are shown in Figure~\ref{fig:O5_GP}. 
The \result{left} panel shows the inferred source frame mass distribution for each of the considered models, and the \result{right} panel shows the corresponding posteriors on \Ho{}.

With \variable{output/num_found_events.txt}events, we find that using a parametric model of the same form as the true simulated distribution (\plp) recovers the injected value of \Ho{} within \result{X-$\sigma$}.
However, when using a model that cannot accurately represent the true shape of the mass distribution, we recover a biased estimate of \Ho: the true value is offset from the mean of the posterior by \result{X}-$\sigma$.
This bias is not due to the need to know the morphology of the mass distribution \emph{a priori}, as the Gaussian process-based model recovers the correct value of \Ho{} despite making minimal assumptions about the mass distribution.
The Gaussian process-based model does, however, obtain larger uncertainties on the inferred value of \Ho, as it has many more free parameters. 
In reality, it is impossible to know the true functional form of the mass distribution, so the additional statistical uncertainty introduced by non-parametric approaches may be desirable over the systematic errors associated with choosing a parametric model.

We have repeated this analysis with \result{50} separate simulated catalogs, and found that the power law model causes us to infer a \result{higher/lower} value of \Ho{} at greater than \result{X}-$\sigma$, \result{Y}\% of the time, whereas the Gaussian process-based model reaches that level of bias only \result{Z}\% of the time, and the \plp{} model reaches the same level of bias \result{A}\% of the time.
This indicates that prior knowledge of the shape of the mass distribution is not required to perform a spectral siren measurement, so long as strong assumptions about the shape of the mass distribution are not made.

\jme{[I think it would be relevant to compare the forecast of the non-parametric and the right parametric model. Do we loose constrianing power by being flexible (probably)? how much?]} \comment{[I have now included this in the discussion.]}

\subsection{Option 2: GP only}
We fit the non-parametric population model described in Section~\ref{sec:model} to a simulated GW catalog of \variable{output/num_found_events.txt}GW events from an underlying mass distribution described by the \plp{} model, with hyperparameters consistent with those found in \citet{o3b_pop}.
This simulated catalog is designed to be representative of the gravitational wave transient catalog after the LVK's fifth observing run.
The details of the data simulation are described in Appendix~\ref{ap:data generation}.

The resulting inferred mass distribution is shown in Figure~\ref{fig:O5_GP}, along with the corresponding posterior on \Ho.
The inferred mass distribution closely resembles the true, simulated distribution, and the injected value of \Ho (\result{67.66 km/s/Mpc}) is consistent with its inferred posterior distribution (\result{X}).
Additionally, the posterior on \Ho is distinct from its prior distribution, indicating that the data is informative despite the flexibility of the population model.

This result also implies that we can expect to measure \Ho{} to \result{X\%} by the end of the LVK's fifth observing run using this method.

\begin{itemize}
    \item If possible, could do constraints on H0 as a function of \# of events.
    \item could also plot H(z) using posterior draws from \Ho{} and \Omm{}- can see at which redshift you get the best constraints. Then plot posterior of $H(z=z_{\text{best measured}})$ rather than \Ho \jme{[I like this option!]}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/O5_GP.pdf}
    \caption{Two-panel figure of mass distribution and \Ho{} posterior. 
    Include prior on \Ho{} as alpha=0.5 line. 
    If we end up fitting \Omm{} we can put a corner plot instead.}
    \label{fig:O5_GP}
    \script{nonparametric_twopanel.py}
\end{figure}


\section{Discussion}
\label{sec:discussion}


\begin{itemize}
    \item We are able to get constraints on cosmological parameters with a very flexible model for the mass distribution. 
    \item The success of the flexible model in recovering an informative and accurate posterior on \Ho demonstrates that there is no need to know the CBC mass distribution's morphology in order to constrain cosmological parameters with it.
    \item This is because the information in spectral sirens comes from assuming events come from the same overall mass distribution across redshift, and that mass distribution doesn't evolve with redshift in the same exact way as would be mimicked by cosmological redshifting. This assumption is very basic, not an assumption that we know the exact astrophyisics of BBH formation (i.e. PPISN feature location).
    \item We have also predicted a \result{X\%} measurement of \Ho{} by the end of the LVK's fifth observing run using non-parametric spectral sirens.
A similar study with parametric methods \citep{Chen:2024gdn} predicts an \result{X\%} measurement, demonstrating a \result{negligible/substantial/marginal} increase in precision.
However, the true population of compact objects in the Universe is unknown, so it is not possible to know the true functional form of the mass distribution, so systematic uncertainties arising from an incorrect choice for the functional form of the mass distribution are therefore inevitable.
With current detectors, these systematic effects are dwarfed by statistical uncertainties.
However, next-generation detectors will herald in enough GW observations to substantially decrease statistical uncertainty in these measurements: \citet{pierra_study_2023} show that incorrect assumptions about the shape of the mass distribution can lead to \result{$\sim3\sigma$} systematic biases in \Ho when catalogs contain 2,000 events.
Thus, the additional statistical uncertainty introduced by non-parametric approaches may be preferable to the systematic errors associated with choosing a parametric model.
    \item future/in prep. work will do the same thing but for a mass distribution that is allowed to evolve with redshift, similarly to the parametric analysis done in~\cite{ezquiaga_spectral_2022}, but with the evolution with redshift allowed to be arbitrary so long as it does not mimic cosmology. - GPs are good bc they can do correlations/generalize to multiple dimensions naturally. Our choice of kernel allows us to do this, but its not possible with the autoregressive kernel.
    \item in XG we will get most bang for our buck with spectral sirens
\end{itemize}
\begin{acknowledgments}
    The authors thank Reed Essick, Utkarsh Mali, Ben Farr, Maya Fishbach,  for helpful conversations.
\end{acknowledgments}

\bibliography{bib}
\appendix
\section{Details of data simulation}
\label{ap:data generation}

We generate a mock catalog of GW events characteristic of that expected from the LVK's fifth observing run (O5). 
These events are drawn from an underlying population described by
\begin{equation}
\label{eq:underlying population}
    \frac{\diff N}{\diff m_1 \diff m_2 \diff z} \propto p(m_1, m_2|\bar{\Lambda}_m) p(z|H_0, \Omega_M),
\end{equation}
where 
\begin{align}
    p(m_1,m_2|\bar{\Lambda}_m) &\propto ,\\
\label{eq:underlying mass dist}
\end{align}
\begin{equation}
    p(z|H_0, \Omega_M) \propto \frac{\diff V_C}{\diff z} \frac{1}{1+z},
    \label{eq:underlying redshift dist}
\end{equation}
$V_C(H_0, \Omega_M)$ is the comoving volume for given cosmological parameters \Ho{} and \Omm{}, and $\bar{\Lambda_m} = \alpha$... are the parameters describing the power law in primary mass, ... respectively. 
When generating the simulated events, we have fixed $\alpha= $... and used cosmological parameters \Ho$=68.7$, \Omm$=0.3$, $\Omega_\Lambda=1-\Omega_M$.
These choices correspond to the maximum \emph{a posteriori} values obtained by an analysis of GWTC-3 data using the \_ model \cite{o3b_pop}, which is morphologically similar to that described in Equations~\ref{eq:underlying population}--\ref{eq:underlying mass and redshift dist}.
We neglect spins entirely, as they do not carry cosmological information.
\comment{TODO: fill in details of mass distribuiton. Should we use PDB and include BNSs, which will give us a nice sharp feature to work with, or should we use PLP which has a bump? Could also do PLP and then put a uniform dist or power law for NS masses at the LMG, and leave an empty mass gap. Would need to get relative heights correct though.}
\jme{[For O5 BNSs will be subdominant. I would focus on BBHs for simplicity]}
\comment{[Ok, so we should say that although a putuative lower mass gap would give and additional feature, it is not expected to be constrained well in O5, so we focus on the BBH spectrum for now. In XG, though, it will be important to consider the full spectrum \citep{ezquiaga_spectral_2022}.]}

After passing the simulated events through projected detector selection effects, the resulting catalog has \variable{output/num_found_events.txt}events, including \result{X} binary neutron star systems, \result{Y} NSBH systems, and \result{Z} BBH systems, consistent with the numbers projected for O5 by \citet{kiendrebogo_observing_2023}.
We use the software package \texttt{GWMockCat} \citep{farah_things_2023} to simulate posterior samples for these events with measurement uncertainties typical of those expected from O5 detectors.
\texttt{GWMockCat} also simulates a set of software injections, which we use to estimate selection effects in the inference.
To determine the detectability of both injections and simulated events in O5, we use the projected O5 LIGO power spectral density \citep{dccpage} to calculate observed signal-to-noise ratios $\rho_{\text{obs}}$, and we consider events and injections with $\rho_{\text{obs}}>8$ to be detectable. 
The full procedure for this mock data generation process is described in \citet{fishbach_where, farah_things_2023, essick_dagnabbit_2023}, and the exact settings used for \texttt{GWMockCat} are made available in the accompanying data release. \comment{TODO:put a showyourwork link here.}

\section{Old}
Old tex that we may end up needing.

\begin{table}[]
    \centering
    \begin{tabular}{c|c c}
         Model & $\sigma_{H_0}$ [km/s/Mpc] & bias [km/s/Mpc]\\
         \hline
         & 
    \end{tabular}
    \caption{Performance of various mass distribution models used for measuring the Hubble constant, \Ho.
    We consider the correct parametric model, an incorrect parametric model, and the non-parametric, Gaussian process-based model presented in this work.
    Bias is defined as the number of standard deviations between the true injected value of \Ho and the recovered posterior mean. 
    The parametric models yield more precise constraints on \Ho{}, as shown by smaller standard deviations ($\sigma_{H_0}$), but the incorrect parametric model is less accurate than the non-parametric model.
    In reality, it is impossible to know the true functional form of the mass distribution, so the additional statistical uncertainty introduced by non-parametric approaches may be desirable over the systematic errors associated with choosing a parametric model.
    }
    \label{tab:bias}
\end{table}

\end{document}
